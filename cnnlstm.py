# -*- coding: utf-8 -*-
"""cnnlstm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eEairAamob2L94p39CarYyl0RAdm2agy
"""

!pip install keras_self_attention

from google.colab import files
uploaded = files.upload()

from keras.models import Model, Sequential
from keras import optimizers
from keras.layers import Input, Conv1D, BatchNormalization, MaxPooling1D, LSTM, Dense, Activation
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.models import load_model
from keras_self_attention import SeqSelfAttention
from datapre import load_data
from keras import initializers
import numpy as np
from keras.utils import to_categorical

def emo1d(input_shape, num_classes, args):
    model = Sequential(name='Emo1D')

    # LFLB1
    model.add(Conv1D(filters=64, kernel_size=3, strides=1, padding='same', input_shape=input_shape,
                     kernel_initializer=initializers.GlorotNormal(seed=42)))
    model.add(BatchNormalization())
    model.add(MaxPooling1D(pool_size=4, strides=4))

    # LFLB2
    model.add(Conv1D(filters=64, kernel_size=3, strides=1, padding='same',
                     kernel_initializer=initializers.GlorotNormal(seed=42)))
    model.add(BatchNormalization())
    model.add(MaxPooling1D(pool_size=4, strides=4))

    # LFLB3
    model.add(Conv1D(filters=128, kernel_size=3, strides=1, padding='same',
                     kernel_initializer=initializers.GlorotNormal(seed=42)))
    model.add(BatchNormalization())
    model.add(MaxPooling1D(pool_size=4, strides=4))

    # LFLB4
    model.add(Conv1D(filters=128, kernel_size=3, strides=1, padding='same',
                     kernel_initializer=initializers.GlorotNormal(seed=42)))
    model.add(BatchNormalization())
    model.add(MaxPooling1D(pool_size=4, strides=4))

    # LSTM layer
    model.add(LSTM(units=args.num_fc, return_sequences=True))
    model.add(SeqSelfAttention(attention_activation='tanh'))
    model.add(LSTM(units=args.num_fc, return_sequences=False))

    # Fully connected layer
    model.add(Dense(units=num_classes, activation='softmax'))

    # Model compilation
    opt = optimizers.Adam(learning_rate=args.learning_rate)
    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])

    return model

def train(model, x_tr, y_tr, x_val, y_val, args):
    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)
    mc = ModelCheckpoint('test_model.h5', monitor='val_accuracy', mode='max', verbose=1,
                         save_best_only=True)
    history = model.fit(x_tr, y_tr, epochs=args.num_epochs, batch_size=args.batch_size, validation_data=(x_val, y_val),
                        callbacks=[es, mc])
    return model

def test(model, x_t, y_t):
    saved_model = load_model('test_model.h5', custom_objects={'SeqSelfAttention': SeqSelfAttention})
    score = saved_model.evaluate(x_t, y_t, batch_size=20)
    print(score)
    return score

def loadData():
    x_tr, y_tr, x_t, y_t, x_val, y_val = load_data()
    x_tr = x_tr.reshape(-1, x_tr.shape[1], 1)
    x_t = x_t.reshape(-1, x_t.shape[1], 1)
    x_val = x_val.reshape(-1, x_val.shape[1], 1)
    y_tr = to_categorical(y_tr)
    y_t = to_categorical(y_t)
    y_val = to_categorical(y_val)
    return x_tr, y_tr, x_t, y_t, x_val, y_val

if __name__ == "__main__":
    class Args:
        num_fc = 128
        batch_size = 32
        num_epochs = 50
        learning_rate = 5e-5
    args = Args()

    x_tr, y_tr, x_t, y_t, x_val, y_val = loadData()
    model = emo1d(input_shape=x_tr.shape[1:], num_classes=len(np.unique(np.argmax(y_tr, axis=1))), args=args)

    model = train(model, x_tr, y_tr, x_val, y_val, args=args)

    score = test(model, x_t, y_t)

import matplotlib.pyplot as plt

# Accuracy and validation accuracy lists
accuracy = [0.3986, 0.5868, 0.6803, 0.7802, 0.8410, 0.8768, 0.9151, 0.9157, 0.9316, 0.9487, 0.9534]
val_acc = [0.3326, 0.3884, 0.5558, 0.6116, 0.6786, 0.6205, 0.7054, 0.6684, 0.7274, 0.7518, 0.7696]

# Epochs (x-axis)
epochs = range(1,len(accuracy) * 5 + 1,5)

# Plotting
plt.figure(figsize=(10, 6))
plt.plot(epochs, accuracy, label='Accuracy', marker='o')
plt.plot(epochs, val_acc, label='Validation Accuracy', marker='s')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy Over Epochs')
plt.legend()
plt.grid(True)
plt.show()

print(score)
print('Final accuracy of the model :',score[1]*100)